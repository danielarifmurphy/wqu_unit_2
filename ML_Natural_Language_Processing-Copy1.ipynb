{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "%logstop\n",
    "%logstart -rtq ~/.logs/ML_Natural_Language_Processing.py append\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "Natural language processing (NLP) is the field devoted to methods and algorithms for processing human (natural) languages for computers. NLP is a vast discipline that is actively being researched. For this notebook, we will be concerned with NLP tools and techniques we can use for machine learning applications. Some examples of machine learning applications using NLP include sentiment analysis, topic modeling, and language translation. In NLP, the following terms have specific meanings:\n",
    "\n",
    "* **Corpus**: The body/collection of text being investigated.\n",
    "* **Document**: The unit of analysis, what is considered a single observation.\n",
    "\n",
    "Examples of corpora include a collection of reviews and tweets, the text of the _Iliad_, and Wikipedia articles. Documents can be whatever you decided, it is what your model will consider an observation. For the example when the corpus is a collection of reviews or tweets, it is logical to make the document a single review or tweet. For the example of the text of the _Iliad_, we can set the document size to a sentence or a paragraph. The choice of document size will be influenced by the size of our corpus. If it is large, it may make sense to call each paragraph a document. As is usually the case, some design choices that need to be made.\n",
    "\n",
    "For this notebook, we will build a classifier to discern homonyms, words that are spelled the same but that have different meanings. The exact use case we will explore is to discern if the word \"python\" refers to the programming language or the animal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP with spaCy\n",
    "\n",
    "spaCy is a Python package that bills itself as \"industrial-strength\" natural language processing. We will use the tools spaCy provides in conjunction with `scikit-learn`. Let's explore some of spaCy's capabilities; we will introduce more functionality when needed. More about spaCy can be found [here](https://spacy.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's try out spacy.\n",
      "We can easily divide our text into sentences!\n",
      "I've run out of ideas.\n",
      "Let\n",
      "We\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# load text processing pipeline (diff to sklearn pipe)\n",
    "nlp = spacy.load('en') # english model\n",
    "\n",
    "# nlp accepts a string\n",
    "doc = nlp(\"Let's try out spacy. We can easily divide our text into sentences! I've run out of ideas.\")\n",
    "\n",
    "# iterate through each sentence\n",
    "for sent in doc.sents:\n",
    "    print(sent) # 3 sentences\n",
    "\n",
    "# index words\n",
    "print(doc[0]) # Let\n",
    "print(doc[6]) # We"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another nice feature from spaCy is part-of-speech tagging, the process of identifying whether a word is a noun, adjective, adverb, etc. A processed word has the attribute `pos_` and `tag_`; the former identifies the simple part of speech (e.g., noun) wile the latter identifies the more detailed part of speech (e.g., proper noun). The meaning of the resulting abbreviations of the `tag_` are listed [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) or can be revealed by running `spacy.explain` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The', 'DET', 'DT')\n",
      "('quick', 'ADJ', 'JJ')\n",
      "('brown', 'ADJ', 'JJ')\n",
      "('fox', 'NOUN', 'NN')\n",
      "('jumped', 'VERB', 'VBD')\n",
      "('over', 'ADP', 'IN')\n",
      "('the', 'DET', 'DT')\n",
      "('lazy', 'ADJ', 'JJ')\n",
      "('dog', 'NOUN', 'NN')\n",
      "('.', 'PUNCT', '.')\n",
      "('Mr.', 'PROPN', 'NNP')\n",
      "('Peanut', 'PROPN', 'NNP')\n",
      "('wears', 'VERB', 'VBZ')\n",
      "('a', 'DET', 'DT')\n",
      "('top', 'ADJ', 'JJ')\n",
      "('hat', 'NOUN', 'NN')\n",
      "('.', 'PUNCT', '.')\n",
      "\n",
      "IN conjunction, subordinating or preposition\n",
      "NNP noun, proper singular\n",
      "NN noun, singular or mass\n",
      "VBD verb, past tense\n",
      "JJ adjective\n",
      "DT determiner\n",
      ". punctuation mark, sentence closer\n",
      "VBZ verb, 3rd person singular present\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"The quick brown fox jumped over the lazy dog. Mr. Peanut wears a top hat.\")\n",
    "tags = set() # set allows us to exploit unique values\n",
    "\n",
    "# reveal part of speech\n",
    "for word in doc:\n",
    "    tags.add(word.tag_)\n",
    "    print((word.text, word.pos_, word.tag_))\n",
    "\n",
    "# revealing meaning of tags\n",
    "print()\n",
    "for tag in tags:\n",
    "    print(tag, spacy.explain(tag)) # explain() method... explains!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining a corpus (get text observations)\n",
    "\n",
    "Before we can move on with our analysis, we need to obtain a corpus. For our intended classifier, we need documents pertaining to python the animal and Python the programming language. Let's use Wikipedia articles to form our corpus. Luckily, there's a Python package called `wikipedia` that makes it easy to fetch articles. We will create documents based on the sentences in the articles. The function allows us to pass multiples pages in constructing the documents, allowing us to prevent one class of documents from dominating the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python is an interpreted, high-level and general-purpose programming language.', \"Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\", 'Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.', 'Python is dynamically typed and garbage-collected.', 'It supports multiple programming paradigms, including structured (particularly, procedural), object-oriented, and functional programming.']\n",
      "\n",
      "['The reticulated python (Malayopython reticulatus) is a species of snake in the family Pythonidae.', 'The species is native to South Asia and Southeast Asia.', \"It is the world's longest snake and listed as least concern on the IUCN Red List because of its wide distribution.\", 'In several range countries, it is hunted for its skin, for use in traditional medicine, and for sale as a pet.', 'It is an excellent swimmer, has been reported far out at sea and has colonized many small islands within its range.\\n']\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "\n",
    "def pages_to_sentences(*pages):\n",
    "    \"\"\"Return a list of sentences in Wikipedia articles.\"\"\"\n",
    "    sentences = []\n",
    "    \n",
    "    for page in pages:\n",
    "        p = wikipedia.page(page)\n",
    "        doc = nlp(p.content)\n",
    "        sentences += [sent.text for sent in doc.sents] # augment list\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "animal_sents = pages_to_sentences(\"Reticulated python\", \"Ball Python\") # use two pages to make balanced dataset!\n",
    "language_sents = pages_to_sentences(\"Python (programming language)\")\n",
    "documents = animal_sents + language_sents\n",
    "\n",
    "print(language_sents[:5])\n",
    "print()\n",
    "print(animal_sents[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "* *Given the example documents, what patterns should our word usage classifier learn?*\n",
    "    - Specific words in documents (sentences) - bag of words model!\n",
    "\n",
    "\n",
    "* *We chose to create documents from sentences. What are other options? What are some pros and cons?*\n",
    "    - Wiki pages - lots of info/signal per document but hard to get lots of training observations!\n",
    "    - Words - too little info/signal per document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The reticulated python (Malayopython reticulatus) is a species of snake in the family Pythonidae.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words model\n",
    "\n",
    "Machine learning models needs to ingest data in a structured form, a matrix where the rows represents observations and the columns are features/attributes. When working with text data, we need a method to convert this unstructured data into a form that the machine learning model can work with. Let's consider our motivating example to create a classifier to discern the usage of \"python\" in a document. We understand that documents referring to the programming language will use words such as \"integer\", \"byte\", and \"error\" at higher frequency than documents that refer to python the animal. The reverse is true for words such as \"bite\", \"snake\", and \"pet\". One technique to _transform_ text data into a matrix is to count the number of appearances of each word in each document. This technique is called the **bag of words** model. The model gets its name because each document is viewed as a bag holding all the words, disregarding word order, context, and grammar. After applying the bag of words model to a corpus, the resulting matrix will exhibit patterns that a machine learning model can exploit. See the example below for the result of applying the bag of words model to a corpus of two documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document 0: \"The python is a large snake, although the snake is not venomous.\" <br>\n",
    "Document 1: \"Python is an interpreted programming language for general purpose programming.\" <br>\n",
    "<br>\n",
    "\n",
    "| although | an | for | general | interpreted | is | language | large | not | programming | purpose | python | snake | the | venomous |\n",
    "|:--------:|----|-----|---------|-------------|----|----------|-------|-----|-------------|---------|--------|-------|-----|----------|\n",
    "|     1    | 0  | 0   | 0       | 0           | 2  | 0        | 1     | 1   | 0           | 0       | 1      | 2     | 2   | 1        |\n",
    "|     0    | 1  | 1   | 1       | 1           | 1  | 1        | 0     | 0   | 2           | 1       | 1      | 0     | 0   | 0        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `CountVectorizer` transformer\n",
    "\n",
    "The bag of words model is found in `scikit-learn` with the `CountVectorizer` transformer. Note, `scikit-learn` uses the word `Vectorizer` to refer to transformers that convert a data structure (like a dictionary) into a NumPy array. Since it is a transformer, we need to first fit the object and _then_ call `transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1004)\t1\n",
      "  (0, 1300)\t1\n",
      "  (0, 1394)\t1\n",
      "  (0, 1577)\t1\n",
      "  (0, 1819)\t1\n",
      "  (0, 2075)\t1\n",
      "  (0, 2078)\t1\n",
      "  (0, 2204)\t1\n",
      "  (0, 2206)\t1\n",
      "  (0, 2409)\t1\n",
      "  (0, 2444)\t1\n",
      "  (0, 2618)\t2\n",
      "  (1, 232)\t1\n",
      "  (1, 283)\t2\n",
      "  (1, 1394)\t1\n",
      "  (1, 1740)\t1\n",
      "  (1, 2433)\t1\n",
      "  (1, 2434)\t1\n",
      "  (1, 2444)\t1\n",
      "  (1, 2618)\t1\n",
      "  (1, 2657)\t1\n",
      "  (2, 232)\t1\n",
      "  (2, 282)\t1\n",
      "  (2, 351)\t1\n",
      "  (2, 616)\t1\n",
      "  :\t:\n",
      "  (624, 90)\t1\n",
      "  (624, 131)\t1\n",
      "  (624, 1395)\t1\n",
      "  (625, 50)\t1\n",
      "  (625, 1603)\t1\n",
      "  (625, 2546)\t1\n",
      "  (626, 70)\t1\n",
      "  (626, 861)\t1\n",
      "  (626, 1300)\t1\n",
      "  (626, 2032)\t1\n",
      "  (626, 2075)\t1\n",
      "  (627, 163)\t1\n",
      "  (627, 2025)\t1\n",
      "  (627, 2822)\t1\n",
      "  (628, 76)\t1\n",
      "  (628, 114)\t1\n",
      "  (628, 131)\t1\n",
      "  (628, 1395)\t1\n",
      "  (629, 299)\t1\n",
      "  (629, 703)\t1\n",
      "  (629, 993)\t1\n",
      "  (629, 1522)\t1\n",
      "  (629, 1824)\t1\n",
      "  (629, 2075)\t1\n",
      "  (629, 2810)\t1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<630x2902 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 9449 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer # transformer to convert data to numpy array\n",
    "\n",
    "bag_of_words = CountVectorizer()\n",
    "bag_of_words.fit(documents) # counts frequencies (column order is arbitrary)\n",
    "word_counts = bag_of_words.transform(documents) # fill in count into appropriate column, generating numpy array with R & C\n",
    "\n",
    "print(word_counts)\n",
    "word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse matrices output (r, c) position of nonzero valued entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "CountVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'check'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.get_feature_names()[500] # col 500 has assigned 'check' to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.vocabulary_['check'] # can find column, given a word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `transform` method returns a sparse matrix (lots of documents (rows) with lots of zeros (not every word is in each sentence)). A sparse matrix is a more efficient manner of storing a matrix. If a matrix has mostly zero entries, it is better to just store the non-zero entries and their occurrence, their row and column. Sparse matrices have the method `toarray()` that returns a full matrix **but** doing so may result in memory issues. Some key hyperparameters of the `CountVectorizer` are shown below:\n",
    "\n",
    "* `min_df`: only counts words that appear in a minimum number of documents.\n",
    "* `max_df`: only counts words that do not appear more than a maximum number of documents.\n",
    "* `max_features`: limits the number of generated features, based on the frequency.\n",
    "\n",
    "After fitting a `CountVectorizer` object, the following method and attribute help with determining which index belongs to which word.\n",
    "\n",
    "* `get_feature_names()`: Returns a list of words used as features. The index of the word corresponds to the column index.\n",
    "* `vocabulary_`: A dictionary mapping a word to its corresponding feature index.\n",
    "\n",
    "Let's use `vocabulary_` to determine how many times \"programming\" occurs in the documents for Python the programming language and python the animal. Do the results make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "# get word counts\n",
    "counts_animal = bag_of_words.transform(animal_sents)\n",
    "counts_language = bag_of_words.transform(language_sents)\n",
    "\n",
    "# index for \"programming\"\n",
    "ind_programming = bag_of_words.vocabulary_['programming']\n",
    "\n",
    "# total counts across all documents\n",
    "print(counts_animal.sum(axis=0)[0, ind_programming]) # does not occur once for Python animal\n",
    "print(counts_language.sum(axis=0)[0, ind_programming]) # occurs many times for Python language!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `HashingVectorizer` transformer\n",
    "\n",
    "The `CountVectorizer` requires that we hold the mapping of words to features in memory. In addition, document processing cannot be parallelized because each worker needs to have the same mapping of word to column index. `CountVectorizer` objects are said to have _state_, they retain information of previous interactions and usage. A trick to improve the `CountVectorizer` is to use a hash function to convert the words into numbers. A hash function is a function that converts an input into a _deterministic_ value. In our context, we will use a hash function to convert a word into a number. The resulting number determines which feature column the word is mapped to. Python has a built-in hash function, seen below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since words are randomly assigned to a column, cannot parallelise due to different mappings. Use deterministic hashing to number (hash value)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7251715771523186212\n",
      "2058740526096236792\n",
      "8389692086922036718\n",
      "7251715771523186212\n"
     ]
    }
   ],
   "source": [
    "print(hash(\"hi!\"))\n",
    "print(hash(\"python\"))\n",
    "print(hash(\"Pyton\"))\n",
    "print(hash(\"hi!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the function returns different values for different words. Also notice, the hash values of \"apple\" and \"apples\" are significantly different. Ideally no two inputs result in the same hash value, but this is impossible to avoid; when different inputs generate the same hash, it is referred to as a \"hash collision\".\n",
    "\n",
    "The `HashingVectorizer` class is similar to the `CountVectorizer` but it uses a hash function to render it *stateless*. The stateless nature of `HashingVectorizer` objects allows it to parallelize the counting process. There are two main disadvantages of `HashingVectorizer`:\n",
    "\n",
    "* Hash collisions are possible but in practice are often inconsequential.\n",
    "* Because the transformer is stateless, there is no mapping between word to feature index (cannot retrieve as before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<630x1048576 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9449 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "hashing_bag_of_words = HashingVectorizer(norm=None) # by default, it normalizes the vectors\n",
    "hashing_bag_of_words.fit(documents)\n",
    "hashing_bag_of_words.transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the feature matrix has over a million columns? This is in contrast from the result of the count vectorizer. The discrepancy is from the `HashingVectorizer` using, by default, $2^{20}=1048576$ different hash values to construct the count matrix. A vast majority of those indices will have no counts across all documents, and since we represent our feature matrix **using a sparse matrix, we pay no cost for empty features!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting time for CountVectorizer: 0.01670050621032715\n",
      "Fitting time for HashingVectorizer: 0.011810302734375\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t_0 = time.time()\n",
    "CountVectorizer().fit_transform(documents)\n",
    "t_elapsed = time.time() - t_0\n",
    "print(\"Fitting time for CountVectorizer: {}\".format(t_elapsed))\n",
    "\n",
    "t_0 = time.time()\n",
    "HashingVectorizer(norm=None).fit_transform(documents)\n",
    "t_elapsed = time.time() - t_0\n",
    "print(\"Fitting time for HashingVectorizer: {}\".format(t_elapsed)) # much quicker!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term frequency-inverse document frequency\n",
    "\n",
    "Using raw counts is limited - common words 'the' too frequent but not important / little signal! Want to *weight* important words. **tf-idf** quantifies this.\n",
    "\n",
    "Both the `CountVectorizer` and `HashingVectorizer` creates a feature matrix of raw counts. Using raw counts has two problems, documents vary widely in length and the counts will be large for common words such as \"the\" and \"is\". We need to use a weighting scheme that considers the aforementioned attributes. The term frequency-inverse document frequency, **tf-idf** for short, is a popular weighting scheme to improve the simple count based data from the bag of words model. It is the product of two values, the term frequency and the inverse document frequency. There are several variants but the most popular is defined below.\n",
    "\n",
    "* **Term Frequency (does job of raw counts, but normalised):**\n",
    "$$ \\mathrm{tf}(t, d) = \\frac{\\mathrm{counts}(t, d)}{\\sqrt{\\sum_{t \\in d} \\mathrm{counts}(t, d)^2}}, $$\n",
    "    where $\\mathrm{counts}(t, d)$ is the raw count of term $t$ in document $d$ and $t \\in d$ are the terms in document $d$. The normalization results in a vector of unit length ($l_2-$norm).\n",
    "\n",
    "* **Inverse Document Frequency (weights according to how ubiquitous counts are *across* documents):**\n",
    "$$ \\mathrm{idf}(t, D) = \\ln\\left(\\frac{\\text{number of documents in corpus } D}{1 + \\text{number of documents with term } t \\,\\text{(in corpus D)}}\\right). $$\n",
    "    Every counted term $t$ in the corpus will have its own idf weight. The $1+$ in the denominator is to ensure no division by zero if a term does not appear in the corpus. The idf weight is simply the log of the inverse of a term's document frequency.\n",
    "    Dealing with large numbers so convention is to take the natrual log.\n",
    "    \n",
    "With both $\\mathrm{tf}(t, d)$ and $\\mathrm{idf}(t, D)$ calculated, the tf-idf weight is\n",
    "\n",
    "$$ \\mathrm{tfidf}(t, d, D) = \\mathrm{tf}(t, d) \\mathrm{idf}(t, D).$$\n",
    "\n",
    "With the idf weighting, words that are very common throughout **many** documents get weighted down ($ln(1)=0$). The reverse is true; the count of rare words get weighted up. With the tf-idf weighting scheme, a machine learning model will have an easier time to learn patterns to properly predict labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to apply the tf-idf weighting in `scikit-learn`, differing in what input they work on. `TfidfVectorizer` works on an array of documents (e.g., list of sentences) while the `TfidfTransformer` works on a count matrix, like the outputs of `HashingVectorizer` and `CountVectorizer`. `TfidfVectorizer` encapsulates the `CountVectorizer` and `TfidfTransformer` into one class. Since we have already calculated the word counts, we will demonstrate the `TfidfTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2618)\t0.22360965933925536\n",
      "  (0, 2444)\t0.3360221624164124\n",
      "  (0, 2409)\t0.24383436811688639\n",
      "  (0, 2206)\t0.3521070976869141\n",
      "  (0, 2204)\t0.24909787207071551\n",
      "  (0, 2078)\t0.43228797466786567\n",
      "  (0, 2075)\t0.1227503092208909\n",
      "  (0, 1819)\t0.1408251425006878\n",
      "  (0, 1577)\t0.3736424077898571\n",
      "  (0, 1394)\t0.1542703951913948\n",
      "  (0, 1300)\t0.1398224604203254\n",
      "  (0, 1004)\t0.43228797466786567\n",
      "  (1, 2657)\t0.12311750055891303\n",
      "  (1, 2618)\t0.10193980617683701\n",
      "  (1, 2444)\t0.3063734742861113\n",
      "  (1, 2434)\t0.35369607640389605\n",
      "  (1, 2433)\t0.35369607640389605\n",
      "  (1, 1740)\t0.3132468150756534\n",
      "  (1, 1394)\t0.1406584512592568\n",
      "  (1, 283)\t0.7073921528077921\n",
      "  (1, 232)\t0.10987173148655972\n",
      "  (2, 2867)\t0.2666734028839735\n",
      "  (2, 2841)\t0.2594048060963993\n",
      "  (2, 2618)\t0.16473800358821106\n",
      "  (2, 2409)\t0.17963797775312407\n",
      "  :\t:\n",
      "  (624, 131)\t0.4465630016054254\n",
      "  (624, 90)\t0.5482531218307405\n",
      "  (624, 64)\t0.5482531218307405\n",
      "  (625, 2546)\t0.644087069184088\n",
      "  (625, 1603)\t0.5567081608993559\n",
      "  (625, 50)\t0.5246216454721525\n",
      "  (626, 2075)\t0.18780958078082602\n",
      "  (626, 2032)\t0.41806983892458516\n",
      "  (626, 1300)\t0.21393019571160277\n",
      "  (626, 861)\t0.5538237967338431\n",
      "  (626, 70)\t0.661406262959921\n",
      "  (627, 2822)\t0.5773502691896257\n",
      "  (627, 2025)\t0.5773502691896257\n",
      "  (627, 163)\t0.5773502691896257\n",
      "  (628, 1395)\t0.4465630016054254\n",
      "  (628, 131)\t0.4465630016054254\n",
      "  (628, 114)\t0.5482531218307405\n",
      "  (628, 76)\t0.5482531218307405\n",
      "  (629, 2810)\t0.45852122942470686\n",
      "  (629, 2075)\t0.13019937170232299\n",
      "  (629, 1824)\t0.4114653765246999\n",
      "  (629, 1522)\t0.4114653765246999\n",
      "  (629, 993)\t0.3963167754473007\n",
      "  (629, 703)\t0.45852122942470686\n",
      "  (629, 299)\t0.2586313772222132\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_weights = tfidf.fit_transform(word_counts)\n",
    "print(tfidf_weights) # sparse TD-IDF matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We no longer have raw counts in our feature matrix. Let's use the `idf_` attribute of the fitted tf-idf transformer to inspect the top idf weights and their corresponding terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.754158681981268 zope\n",
      "6.754158681981268 guinea\n",
      "6.754158681981268 handle\n",
      "6.754158681981268 hamilton\n",
      "6.754158681981268 hall\n",
      "6.754158681981268 habitation\n",
      "6.754158681981268 g√∂ttingen\n",
      "6.754158681981268 gutted\n",
      "6.754158681981268 guide\n",
      "6.754158681981268 info\n",
      "6.754158681981268 guard\n",
      "6.754158681981268 grumpy\n",
      "6.754158681981268 grows\n",
      "6.754158681981268 grown\n",
      "6.754158681981268 growing\n",
      "6.754158681981268 groovy\n",
      "6.754158681981268 handled\n",
      "6.754158681981268 handling\n",
      "6.754158681981268 happen\n"
     ]
    }
   ],
   "source": [
    "top_idf_indices = tfidf.idf_.argsort()[:-20:-1] # indices of top 20\n",
    "ind_to_word = bag_of_words.get_feature_names()\n",
    "\n",
    "for ind in top_idf_indices:\n",
    "    print(tfidf.idf_[ind], ind_to_word[ind])\n",
    "    \n",
    "# these are not common *across* documents, so weighted upward most"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using tf-idf weighting renders the process as _stateful_; to apply the idf weight, we need to know the frequency of each word across all documents. While we may initially use `HasingVectorizer` to have a stateless transformer, coupling it with `TfidfTransformer` will create a stateful process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving signal\n",
    "\n",
    "So far, we have discussed how using tf-idf rather than raw counts will improve the performance of our machine learning model. There are several other approaches that can boost performance; we will discuss techniques that improve the signal in our data set. Note, the following techniques may marginally increase model performance. \n",
    "\n",
    "\n",
    "*It may be best to create a baseline model and measure the increased performance with the new model additions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words (equivalent of LASSO...)\n",
    "\n",
    "Words such as \"the\", \"a\", and \"or\" are so common throughout our corpus that they do not contribute any signal to our data set. Further, omitting these words will **reduce our already high dimensional data set**. It is best to not have these words as features and not be counted in the analysis. The set of words that will not factor into our analysis are called **stop words**.\n",
    "\n",
    "spaCy provides a `set` of around 300 commonly used English words. When using stop words, it is best to examine the entries in case there are certain words you want to be included or not included. Since the words are provided as a Python `set`, we can use methods available to `set` objects to modify entries of the `set` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'set'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amount',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'are',\n",
       " 'around',\n",
       " 'as',\n",
       " 'at',\n",
       " 'back',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'bottom',\n",
       " 'but',\n",
       " 'by',\n",
       " 'ca',\n",
       " 'call',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'could',\n",
       " 'did',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doing',\n",
       " 'done',\n",
       " 'down',\n",
       " 'due',\n",
       " 'during',\n",
       " 'each',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'eleven',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'empty',\n",
       " 'enough',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'except',\n",
       " 'few',\n",
       " 'fifteen',\n",
       " 'fifty',\n",
       " 'first',\n",
       " 'five',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forty',\n",
       " 'four',\n",
       " 'from',\n",
       " 'front',\n",
       " 'full',\n",
       " 'further',\n",
       " 'get',\n",
       " 'give',\n",
       " 'go',\n",
       " 'had',\n",
       " 'has',\n",
       " 'have',\n",
       " 'he',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'however',\n",
       " 'hundred',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'indeed',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'keep',\n",
       " 'last',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'made',\n",
       " 'make',\n",
       " 'many',\n",
       " 'may',\n",
       " 'me',\n",
       " 'meanwhile',\n",
       " 'might',\n",
       " 'mine',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'move',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'name',\n",
       " 'namely',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'part',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'please',\n",
       " 'put',\n",
       " 'python',\n",
       " 'quite',\n",
       " 'rather',\n",
       " 're',\n",
       " 'really',\n",
       " 'regarding',\n",
       " 'same',\n",
       " 'say',\n",
       " 'see',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'serious',\n",
       " 'several',\n",
       " 'she',\n",
       " 'should',\n",
       " 'show',\n",
       " 'side',\n",
       " 'since',\n",
       " 'six',\n",
       " 'sixty',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhere',\n",
       " 'still',\n",
       " 'such',\n",
       " 'take',\n",
       " 'ten',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " 'third',\n",
       " 'this',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'top',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'twelve',\n",
       " 'twenty',\n",
       " 'two',\n",
       " 'under',\n",
       " 'unless',\n",
       " 'until',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'used',\n",
       " 'using',\n",
       " 'various',\n",
       " 'very',\n",
       " 'via',\n",
       " 'was',\n",
       " 'we',\n",
       " 'well',\n",
       " 'were',\n",
       " 'what',\n",
       " 'whatever',\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whither',\n",
       " 'who',\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'would',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en import STOP_WORDS\n",
    "\n",
    "print(type(STOP_WORDS))\n",
    "STOP_WORDS_python = STOP_WORDS.union({\"python\"}) # adds python to set of stop words\n",
    "STOP_WORDS_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'languange' in STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and lemmatization\n",
    "\n",
    "In our current analysis, words like \"python\" and \"pythons\" will be counted as separate words. We understand that they represent the same concept and want them to be treated as the same word. The same applies to other words like \"run\", \"runs\", \"ran\", and \"running\", they all represent the same meaning. **Stemming** is the process of reducing a word to its stem. Note, the stemming process is not 100% effective and sometimes the resulting stem is not an actual word. For example, the popular Porter stemming algorithm applied to \"argues\" and \"arguing\" returns `\"argu\"`.\n",
    "\n",
    "**Lemmatization** is the process of reducing a word to its lemma, or the dictionary form of the word. It is a more sophisticated process than stemming as it considers context and part of speech. Further, the resulting lemma is an actual word. spaCy does not have a stemming algorithm but does offer lemmatization. Each word analyzed by spaCy has the attribute `lemma_` which returns the lemma of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'run', 'run', 'run']\n",
      "['buy', 'buy', 'buying', 'buy']\n",
      "['see', 'saw', 'see', 'see']\n"
     ]
    }
   ],
   "source": [
    "print([word.lemma_ for word in nlp('run runs ran running')])\n",
    "print([word.lemma_ for word in nlp('buy buys buying bought')])\n",
    "print([word.lemma_ for word in nlp('see saw seen seeing')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: As of version 2.0.16 of spaCy, there is the bug with the English lemmatization and will fail in instances it should not. However, the bug has been fixed and a patch will be included in a future update, version 2.1.x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply lemmatization in `scikit-learn`, you need to pass a function to the keyword `tokenizer` of whatever text vectorizer you are deploying. See the example below were we apply lemmatization for a `TfidfVectorizer` transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "CountVectorizer?\n",
    "# tokenizer sets the token, or length of item of interest in document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['-pron-'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', '\\n\\n', '\\n\\n\\n', ' ', '\"', \"'\", \"'s\", '(', ')', ',', '-', '.', '1', '2', '3', ':', ';', '<', '=', 'allow', 'ball', 'block', 'body', 'breed', 'c', 'captivity', 'care', 'class', 'code', 'common', 'compile', 'cpython', 'design', 'development', 'division', 'e.g.', 'eat', 'enclosure', 'example', 'expression', 'feature', 'female', 'find', 'ft', 'function', 'good', 'help', 'human', 'implementation', 'include', 'island', 'java', 'kill', 'language', 'large', 'later', 'length', 'library', 'like', 'list', 'live', 'long', 'm', 'm.', 'male', 'method', 'module', 'new', 'number', 'object', 'old', 'operator', 'pattern', 'pet', 'prey', 'program', 'programming', 'provide', 'r.', 'range', 'reference', 'release', 'report', 'reticulate', 'reticulated', 'small', 'snake', 'standard', 'statement', 'string', 'support', 'syntax', 'system', 'time', 'type', 'value', 'variable', 'version', 'write', 'year']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# want to use our own lemmatizer function\n",
    "def lemmatizer(text):\n",
    "    return [word.lemma_ for word in nlp(text)]\n",
    "\n",
    "# we need to generate the lemmas of the stop words\n",
    "stop_words_str = \" \".join(STOP_WORDS) # nlp function needs a string\n",
    "stop_words_lemma = set(word.lemma_ for word in nlp(stop_words_str))\n",
    "\n",
    "# using tfidf VECTORISER since not inputting array of raw counts\n",
    "tfidf_lemma = TfidfVectorizer(max_features=100, \n",
    "                              stop_words=stop_words_lemma.union({\"python\"}),\n",
    "                              tokenizer=lemmatizer) # want our tokens to be word lemmas! \n",
    "\n",
    "tfidf_lemma.fit(documents)\n",
    "print(tfidf_lemma.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog', 'be', 'run', 'quickly', '.']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer('Dogs are running quickly.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and n-grams\n",
    "\n",
    "Tokenization refers to dividing up a document into pieces to be counted. In our analysis so far, we are only counting words. However, it may be useful to count a sequence of words such as \"natural environment\" and \"virtual environment\". Counting these **bigrams** for our word usage analyzer may boost performance. More generally, an n-gram refers to the n sequence of words. In `scikit-learn`, n-grams can be included by setting `ngram_range=(min_n, max_n)` for the vectorizer, where `min_n` and `max_n` are the lower and upper bound of the range of n-grams to include. For example, `ngram_range=(1, 2)` will include words and bigrams while `ngram_range=(2, 2)` will only count bigrams. Let's see what are the most frequent bigrams in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['23 ft',\n",
       " 'ball pythons',\n",
       " 'floating point',\n",
       " 'ft length',\n",
       " 'ft long',\n",
       " 'guido van',\n",
       " 'isbn 978',\n",
       " 'list comprehensions',\n",
       " 'number incremented',\n",
       " 'object oriented',\n",
       " 'oriented programming',\n",
       " 'programming language',\n",
       " 'programming languages',\n",
       " 'reference implementation',\n",
       " 'reticulated pythons',\n",
       " 'scripting language',\n",
       " 'standard library',\n",
       " 'van rossum',\n",
       " 'version number',\n",
       " 'year old']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_counter=CountVectorizer(max_features=20, ngram_range=(2,2),        # only length 2\n",
    "                               stop_words=STOP_WORDS.union({\"python\"}))\n",
    "bigram_counter.fit(documents)\n",
    "\n",
    "bigram_counter.get_feature_names() # most frequent bigrams!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "* Is using stop words more important when using `CountVectorizer`/`HashingVectorizer` or when using the `TfidfVectorizer`?\n",
    "    - more important when using former as these have high frequencies across all documents and otherwise would not be weighted downward. Note that, even with Tfidf, good to include stop words anyway since it is cheap!\n",
    "\n",
    "\n",
    "* Is it practical to use a large n-gram range, for example, count 3-grams?\n",
    "    - curse of dimensionality + overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document similarity\n",
    "\n",
    "After we have transformed our corpus into a matrix, we can interpret our data set as representing a set of vectors in a $p$-dimensional space, where each document is its own vector. One common analysis is to find similar documents. The cosine similarity is a metric that measure how well aligned in space are two vectors, equal to the cosine of the angle in between the two vectors. If the vectors are perfectly aligned, they point in the same direction, the angle they form is 0 and the similarity score is 1. If the vectors are orthogonal, forming an angle of 90 degrees, the similarity metric is 0. Mathematically, the cosine similarity metric is equal to the dot product of two vectors, normalized,\n",
    "\n",
    "$$ \\frac{v_1 \\cdot v_2}{\\|v_1 \\|\\|v_2 \\|}, $$\n",
    "\n",
    "where $v_1$ and $v_2$ are two document vectors and $\\| v_1 \\|$ and $\\| v_2 \\|$ are their lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word usage classifier\n",
    "\n",
    "Let's build a word usage classifier with all the techniques we have seen. The model will include:\n",
    "\n",
    "* tf-idf weighting\n",
    "* stop words\n",
    "* words and bigrams\n",
    "* lemmatization\n",
    "\n",
    "Applying the above techniques should result in a data set with enough signal that a machine learning model can learn from. For this exercise, we will use the naive Bayes model; a probabilistic model that calculates conditional probabilities using Bayes theorem. The term naive is applied because it assumes the features are conditionally independent from each other. You can think of a naive Bayes classifier working by determining what class should a document be assigned based upon the frequencies of words in the different classes in the training set. Naive Bayes is often used as benchmark model for NLP as it is quick to train. More about the model in general can be found [here](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) and details of the `scikit-learn` implementation is found [here](https://scikit-learn.org/stable/modules/naive_bayes.html). After training our model, we will see how well it performs for a chosen set of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['-pron-'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9888888888888889\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# create data set and labels\n",
    "documents = animal_sents + language_sents\n",
    "labels = [\"animal\"]*len(animal_sents) + [\"language\"]*len(language_sents)\n",
    "\n",
    "# lemma of stop words\n",
    "stop_words_str = \" \".join(STOP_WORDS)\n",
    "stop_words_lemma = set(word.lemma_ for word in nlp(stop_words_str))\n",
    "\n",
    "# create and train pipeline\n",
    "tfidf = TfidfVectorizer(stop_words=stop_words_lemma, tokenizer=lemmatizer, ngram_range=(1, 2))\n",
    "pipe = Pipeline([('vectorizer', tfidf), ('classifier', MultinomialNB())])\n",
    "pipe.fit(documents, labels)\n",
    "\n",
    "print(\"Training accuracy: {}\".format(pipe.score(documents, labels))) # very high train set accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Python program is only 100 bytes long. --> language at 69.5214%\n",
      "A python's bite is not venomous but still hurts. --> language at 52.8553%\n",
      "I can't find the error in the python code. --> language at 69.8606%\n",
      "Where is my pet python; I can't find her! --> animal at 60.3494%\n",
      "I use for and while loops when writing Python. --> language at 81.3527%\n",
      "The python will loop and wrap itself onto me. --> language at 63.369%\n",
      "I use snake case for naming my variables. --> language at 55.8552%\n",
      "My python has grown to over 10 ft long! --> animal at 64.4591%\n",
      "I use virtual environments to manage package versions. --> language at 73.6893%\n",
      "Pythons are the largest snakes in the environment. --> animal at 75.4041%\n"
     ]
    }
   ],
   "source": [
    "test_docs = [\"My Python program is only 100 bytes long.\",\n",
    "             \"A python's bite is not venomous but still hurts.\",\n",
    "             \"I can't find the error in the python code.\",\n",
    "             \"Where is my pet python; I can't find her!\",\n",
    "             \"I use for and while loops when writing Python.\",\n",
    "             \"The python will loop and wrap itself onto me.\",\n",
    "             \"I use snake case for naming my variables.\",\n",
    "             \"My python has grown to over 10 ft long!\",\n",
    "             \"I use virtual environments to manage package versions.\",\n",
    "             \"Pythons are the largest snakes in the environment.\"]\n",
    "\n",
    "class_labels = [\"animal\", \"language\"]\n",
    "y_proba = pipe.predict_proba(test_docs)\n",
    "predicted_indices = (y_proba[:, 1] > 0.5).astype(int)\n",
    "\n",
    "for i, index in enumerate(predicted_indices):\n",
    "    print(test_docs[i], \"--> {} at {:g}%\".format(class_labels[index], 100*y_proba[i, index])) # gets some wrong!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Encapsulate the entire process of gathering a corpus, constructing, and training a model into a function. Afterwards, deploy the model to other sets of homonyms.\n",
    "1. Measure the model's improvements by stripping out things such as the use of stop words and lemmatization. Perhaps you can incorporate model additions as parameters to the previously mentioned function. What model additions increases the performance the most?\n",
    "1. Consider another source of data and see how well the model performs with the new corpus.\n",
    "1. Naive Bayes classifier calculates conditional probabilities from the training set. In other words, it determines values like $P(\\text{snake | }Y = \\text{animal})$, the probability a document has the word \"snake\" given if the document belongs to those of python the animal. These values are stored in `coef_` attribute of a trained naive Bayes model. Can you use these coefficients to determine the most discriminative features? In other words, what terms when found in a document really help classify the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Encapsulate the entire process of gathering a corpus, constructing, and training a model into a function. Afterwards, deploy the model to other sets of homonyms.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(words):\n",
    "    corpus = []\n",
    "    labels = []\n",
    "    \n",
    "    for k, v in words.items():\n",
    "        documents = pages_to_sentences(*v) # pass string refering to page and returns sentences in page; *args unpacking\n",
    "        corpus += documents\n",
    "        labels += [k] * len(documents)\n",
    "        \n",
    "    return corpus, labels\n",
    "        \n",
    "        \n",
    "        \n",
    "words = {'greek': ['Amazons'],\n",
    "         'rainforest': ['Amazon_rainforest'],\n",
    "         'company': ['Amazon_(company)']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, labels = get_corpus(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'greek',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'rainforest',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " 'company',\n",
       " ...]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Measure the model's improvements by stripping out things such as the use of stop words and lemmatization. Perhaps you can incorporate model additions as parameters to the previously mentioned function. What model additions increases the performance the most?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1, 1) or (1, 2)\n",
    "# lemmatise or don't\n",
    "\n",
    "# 4 combinations! Use GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(documents, labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vectorizer',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop...\n",
       "                                                        tokenizer=None,\n",
       "                                                        use_idf=True,\n",
       "                                                        vocabulary=None)),\n",
       "                                       ('clf',\n",
       "                                        MultinomialNB(alpha=1.0,\n",
       "                                                      class_prior=None,\n",
       "                                                      fit_prior=True))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'vectorizer__tokenizer': [None,\n",
       "                                                   <function lemmatizer at 0x7f9b8c8d3950>]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([('vectorizer', TfidfVectorizer()), ('clf', MultinomialNB())])\n",
    "\n",
    "param_grid = {'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "              'vectorizer__tokenizer': [None, lemmatizer]}\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv=5, verbose=1)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.01404395, 5.90101838, 0.02765331, 5.92580495]),\n",
       " 'std_fit_time': array([0.00240042, 0.08325635, 0.00049656, 0.03145234]),\n",
       " 'mean_score_time': array([0.00295777, 1.48100672, 0.00425415, 1.48049474]),\n",
       " 'std_score_time': array([3.76309130e-04, 2.30910991e-02, 8.35166908e-05, 1.90013696e-02]),\n",
       " 'param_vectorizer__ngram_range': masked_array(data=[(1, 1), (1, 1), (1, 2), (1, 2)],\n",
       "              mask=[False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vectorizer__tokenizer': masked_array(data=[None, <function lemmatizer at 0x7f9b8c8d3950>, None,\n",
       "                    <function lemmatizer at 0x7f9b8c8d3950>],\n",
       "              mask=[False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'vectorizer__ngram_range': (1, 1), 'vectorizer__tokenizer': None},\n",
       "  {'vectorizer__ngram_range': (1, 1),\n",
       "   'vectorizer__tokenizer': <function __main__.lemmatizer(text)>},\n",
       "  {'vectorizer__ngram_range': (1, 2), 'vectorizer__tokenizer': None},\n",
       "  {'vectorizer__ngram_range': (1, 2),\n",
       "   'vectorizer__tokenizer': <function __main__.lemmatizer(text)>}],\n",
       " 'split0_test_score': array([0.88235294, 0.87254902, 0.85294118, 0.84313725]),\n",
       " 'split1_test_score': array([0.91089109, 0.9009901 , 0.88118812, 0.84158416]),\n",
       " 'split2_test_score': array([0.9009901 , 0.88118812, 0.88118812, 0.85148515]),\n",
       " 'split3_test_score': array([0.91, 0.91, 0.87, 0.9 ]),\n",
       " 'split4_test_score': array([0.95, 0.9 , 0.9 , 0.9 ]),\n",
       " 'mean_test_score': array([0.91071429, 0.89285714, 0.87698413, 0.86706349]),\n",
       " 'std_test_score': array([0.02209228, 0.01387229, 0.01545496, 0.02692708]),\n",
       " 'rank_test_score': array([1, 2, 3, 4], dtype=int32)}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = grid_search.cv_results_\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.91071429, 0.89285714, 0.87698413, 0.86706349])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['mean_test_score'] # 4 different combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vectorizer__ngram_range': (1, 1), 'vectorizer__tokenizer': None}\n",
      "0.9107142857142857\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_) # 1 grams and no tokenizer is the best (due to overfitting)!\n",
    "# want to take this into account AND the difference in training times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Consider another source of data and see how well the model performs with the new corpus.**\n",
    "\n",
    "Let's use Reddit! Subreddit gives you the labels. Luckily, there is a Reddit API called PRAW to pull out all the Reddit post data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Naive Bayes classifier calculates conditional probabilities from the training set. In other words, it determines values like $P(\\text{snake | }Y = \\text{animal})$, the probability a document has the word \"snake\" given if the document belongs to those of python the animal. These values are stored in `coef_` attribute of a trained naive Bayes model. Can you use these coefficients to determine the most discriminative features? In other words, what terms when found in a document really help classify the document.**\n",
    "\n",
    "Can compare via the log ratio:\n",
    "$$ log \\left( \\frac{P(snake | animal}{P(snake | language)} \\right) $$.\n",
    "\n",
    "Large ratio = discriminative! If positive, discriminative towards animal; if negative, towards language."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
