{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interfacing with `GridSearchCV` and `Pipeline`\n",
    "The syntax to interface `GridSearchCV` and a pipeline estimator can be tricky. There are two methods, each with its pros and cons. The first method feeds a pipeline estimator into `GridSearchCV` while the second has `GridSearchCV` as a step in the pipeline. Remember, both `Pipeline` and `GridSearchCV` are estimator classes, with the `fit`, `predict`, and `score` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demostration, let's use the California housing dataset and create a `Pipeline` object containing two steps: a feature scaler and a ridge predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5943141338604155\n"
     ]
    }
   ],
   "source": [
    "# Get data and perform train/test split\n",
    "X = fetch_california_housing()['data']\n",
    "y = fetch_california_housing()['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Create pipeline estimator\n",
    "pipe_1 = Pipeline([('scaler', StandardScaler()), ('regressor', Ridge())])\n",
    "pipe_1.fit(X_train, y_train)\n",
    "\n",
    "print(pipe_1.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a gridseach on a pipeline\n",
    "The first method performs a gridsearch on a `Pipeline` estimator. A `Pipeline` object absorbs the attributes and parameters of all the steps, the transformers and the final predictor, and uses the name of the step as the prefix to the name of the attributes. For example, the ridge regularization parameter is now referred to as `regressor__alpha` rather than just `alpha`. The prefix is needed distinguish between the hyperparameters among all the estimators of the pipeline. The `get_params` method returns a dictionary of the pipeline parameters, using the name of the stage and double underscore (dunder) as a prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('scaler',\n",
       "   StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "  ('regressor',\n",
       "   Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "         normalize=False, random_state=None, solver='auto', tol=0.001))],\n",
       " 'verbose': False,\n",
       " 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'regressor': Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "       normalize=False, random_state=None, solver='auto', tol=0.001),\n",
       " 'scaler__copy': True,\n",
       " 'scaler__with_mean': True,\n",
       " 'scaler__with_std': True,\n",
       " 'regressor__alpha': 1.0,\n",
       " 'regressor__copy_X': True,\n",
       " 'regressor__fit_intercept': True,\n",
       " 'regressor__max_iter': None,\n",
       " 'regressor__normalize': False,\n",
       " 'regressor__random_state': None,\n",
       " 'regressor__solver': 'auto',\n",
       " 'regressor__tol': 0.001}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print dictionary of pipeline parameters\n",
    "pipe_1.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform a gridsearch on the ridge regularization hyperparameter, the keyword `regressor__alpha` needs to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5941973302098567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  60 out of  60 | elapsed:    2.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Perform hyperparameter tuning on pipeline estimator\n",
    "param_grid = {'regressor__alpha': np.logspace(-3, 3, 20)} # 10^-3 to 10^3\n",
    "gs_est = GridSearchCV(pipe_1, param_grid, cv=3, n_jobs=2, verbose=1) # GS of pipe\n",
    "gs_est.fit(X_train, y_train)\n",
    "\n",
    "print(gs_est.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'regressor__alpha': 12.742749857031322}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_est.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of feeding a `pipeline` object to `GridSearchCV` is that it allows for tuning all the hyperparameters of the steps in the pipeline, not just the final predictor. For example, if we wanted to explore the effect of modifying the `with_mean` parameter of the scaler transformer, we would use the keyword `scaler__with_mean` since we used the string \"scaler\" to refer to the scaler transformer when creating our pipeline.\n",
    "\n",
    "When calling the `fit` method, each training step of the cross-validation scheme will perform the fit and transformations on all steps in the pipeline and not just the final predictor. If the gridsearch is not searching through the hyperparameters of the transformers, needless computations are being performed which may come with a larger runtime of the gridsearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `GridSearchCV` inside the pipeline\n",
    "`GridSearchCV` can be the final step of the pipeline. Since `GridSearchCV` is only operating on an estimator that is not a pipeline, the name of hyperparameters is simply the original name from the estimator. For example, to modify the regularization hyperparameter of the ridge estimator, the string to that refers to this hyperparameter is simply \"alpha\". The disadvantage of this approach is that you can only search through the hyperparameters of the estimator that was fed to `GridSearchCV` and not of any of the other transformers. However, since the gridsearch only starts at the last step of the pipeline, the transformations of the dataset occurs once, which is computationally more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5941973302098567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Include gridsearch estimator inside of the pipeline\n",
    "param_grid = {'alpha': np.logspace(-3, 3, 20)}\n",
    "gs = GridSearchCV(Ridge(), param_grid) # GS only for predictor\n",
    "pipe_2 = Pipeline([('scaler', StandardScaler()), ('gs_est', gs)])\n",
    "pipe_2.fit(X_train, y_train)\n",
    "\n",
    "print(pipe_2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scaler': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'gs_est': GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "              estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True,\n",
       "                              max_iter=None, normalize=False, random_state=None,\n",
       "                              solver='auto', tol=0.001),\n",
       "              iid='warn', n_jobs=None,\n",
       "              param_grid={'alpha': array([1.00000000e-03, 2.06913808e-03, 4.28133240e-03, 8.85866790e-03,\n",
       "        1.83298071e-02, 3.79269019e-02, 7.84759970e-02, 1.62377674e-01,\n",
       "        3.35981829e-01, 6.95192796e-01, 1.43844989e+00, 2.97635144e+00,\n",
       "        6.15848211e+00, 1.27427499e+01, 2.63665090e+01, 5.45559478e+01,\n",
       "        1.12883789e+02, 2.33572147e+02, 4.83293024e+02, 1.00000000e+03])},\n",
       "              pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "              scoring=None, verbose=0)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_2.named_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching transformers of a pipeline\n",
    "\n",
    "### Don't want to perform StandardScaler() every time a new hyperparameter value is fit.\n",
    "\n",
    "As of version 0.19, scikit-learn has given `Pipeline` a way to cache transformers. Caching allows unneccessarily fitting transformers; if a transformer with a set of arguments has already been fitted, it would have been cached when initially fitted and is simply loaded into memory. The caching of transformers makes the hyperparameter tuning process run faster and one can perform the search on the pipeline rather than have the search as the final step. Note, that caching transformers will only be faster if it's computationally expensive to fit the tranformer. You may not see any improvements if you are using something like `StandardScaler`, which is fast to fit.\n",
    "\n",
    "To cache the transformers of a pipeline, you need to use the `memory` keyword argument and set it equal to a string referring to the path of the caching directory. We can make a temporary directing using `mkdtemp` and remove it after use with `rmtree`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "\n",
    "cachedir = mkdtemp()\n",
    "pipe_cache = Pipeline([('scaler', StandardScaler()), ('regressor', Ridge())], memory=cachedir)\n",
    "pipe_cache.fit(X_train, y_train)\n",
    "rmtree(cachedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `RandomizedSearchCV` as an alternative to `GridSearchCV`\n",
    "\n",
    "The number of gridpoints to explore when using `GridSearchCV` grows exponentially with the number of different types of hyperparameters you are exploring. You can consider each hyperparameter as a dimension in a n-dimensional space. An alternate to `GridSearhCV` is `RandomizedSearchCV` which constructs the grid of hyperparameters but randomly samples from the grid. The number of grid points sampled is controlled by the `n_iter` keyword. It has been shown that a randomized search can perform just as well as a full gridsearch but with only a fraction of the runtime. A \"savings\" of computational time can then be \"invested\" into exploring other hyperparameters or using a larger grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5942975332161693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  60 out of  60 | elapsed:    1.9s finished\n"
     ]
    }
   ],
   "source": [
    "# Use randomized search\n",
    "param_grid = {'regressor__alpha': np.logspace(-4, 4, 100)}\n",
    "gs_est = RandomizedSearchCV(pipe_1, param_grid, cv=3, n_jobs=2, verbose=1, random_state=0, n_iter=20)\n",
    "gs_est.fit(X_train, y_train)\n",
    "\n",
    "print(gs_est.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `dill` to serialize estimators\n",
    "`pickle` is a Python standard library that serializes Python objects. Serialization is the process of converting a data structure or object into a byte stream that can stored to disk. Deserialization refers to the opposite process where a byte stream is constructed to form a data structure or object. Think of pickling as a way to save Python objects to disk so they can be loaded and used at a later time. The `pickle` library has its limitations on what objects it can successfully serialize. Fortunately, the `dill` library improves on `pickle` by extending the set of possible Python data types that can be serialize. The syntax between `pickle` and `dill` are nearly identical.\n",
    "\n",
    "It is useful to serialize scikit-learn estimators, especially after they have been trained. To serialize our trained estimator, we simply open a file and call `dill.dump` on the file object. Remember, it is good practice to use the `with` statement when dealing with file objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "with open('pipe_of_ridge.dill', 'wb') as fp:\n",
    "    dill.dump(pipe_1, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, when using `open`, the `w` signafies that we want write access to the file. The file extension can be anything, however, using \"dill\" is descriptive and a nice convention. To deserialize the estimator, use `load` on the dill file. Note that `r` is used with `open` because we need read access to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pipe_of_ridge.dill', 'rb') as fp:\n",
    "    pipe_1 = dill.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the estimator loaded in memory and is ready to be used. If you are curious about the memory requirements of different trained estimators, you can use `dill` and check the resulting file size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipe_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-31bde285ecca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pipe_1' is not defined"
     ]
    }
   ],
   "source": [
    "print(pipe_1.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** scikit-learn's preferred way to serialize models is using [`Joblib`](https://joblib.readthedocs.io/en/latest/index.html). You can read more [here](https://scikit-learn.org/stable/modules/model_persistence.html) but it pretty works the same as `pickle` and `dill`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persisting the Model\n",
    "Let's use a library to save our model.\n",
    "**Note:** Do not 'open' dill file from stranger!\n",
    "\n",
    "`pickle`\n",
    "\n",
    "`dill` - can saved more object types than pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_model.dill', 'wb') as f:  # .dill is just convention to remind, but could name as anything; f is file handle\n",
    "    dill.dump(model, f)                    # (model name, f); saves dill file in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_model.dill', 'rb') as f:  # rb = read binary\n",
    "    saved_model = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model.predict(X_test)   # already fit! Can predict straight away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could also dump e.g. training data!\n",
    "\n",
    "with open('training_data.dill', 'wb') as f:\n",
    "    dill.dump(X_train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_model.dill', 'rb') as f:  # can load training data df!\n",
    "    X_train = dill.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
