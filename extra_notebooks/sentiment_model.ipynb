{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/datacourse/extra_notebooks/data/Sentiment-Analysis-Dataset.zip'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.abspath('data/Sentiment-Analysis-Dataset.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 8836: expected 4 fields, saw 5\\n'\n",
      "b'Skipping line 535882: expected 4 fields, saw 7\\n'\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/home/jovyan/datacourse/extra_notebooks/data/Sentiment-Analysis-Dataset.zip',\n",
    "                 compression='zip', error_bad_lines=False)\n",
    "# only 2 rows were bad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                          is so sad for my APL frie...\n",
       "1                        I missed the New Moon trail...\n",
       "2                               omg its already 7:30 :O\n",
       "3               .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4              i think mi bf is cheating on me!!!   ...\n",
       "5                     or i just worry too much?        \n",
       "6                    Juuuuuuuuuuuuuuuuussssst Chillin!!\n",
       "7            Sunny Again        Work Tomorrow  :-|  ...\n",
       "8           handed in my uniform today . i miss you ...\n",
       "9              hmmmm.... i wonder how she my number @-)\n",
       "10                        I must think about positive..\n",
       "11          thanks to all the haters up in my face a...\n",
       "12                       this weekend has sucked so far\n",
       "13               jb isnt showing in australia any more!\n",
       "14                                 ok thats it you win.\n",
       "15        &lt;-------- This is the way i feel right ...\n",
       "16        awhhe man.... I'm completely useless rt no...\n",
       "17        Feeling strangely fine. Now I'm gonna go l...\n",
       "18         HUGE roll of thunder just now...SO scary!!!!\n",
       "19        I just cut my beard off. It's only been gr...\n",
       "20                                 Very sad about Iran.\n",
       "21                                        wompppp wompp\n",
       "22        You're the only one who can see this cause...\n",
       "23       &lt;---Sad level is 3. I was writing a mass...\n",
       "24       ...  Headed to Hospitol : Had to pull out o...\n",
       "25       BoRinG   ): whats wrong with him??     Plea...\n",
       "26       can't be bothered. i wish i could spend the...\n",
       "27       Feeeling like shit right now. I really want...\n",
       "28                goodbye exams, HELLO ALCOHOL TONIGHT \n",
       "29       I didn't realize it was THAT deep. Geez giv...\n",
       "30       I hate it when any athlete appears to tear ...\n",
       "31       i miss you guys too     i think i'm wearing...\n",
       "32               -- Meet your Meat http://bit.ly/15SSCI\n",
       "33             My horsie is moving on Saturday morning.\n",
       "34             No Sat off...Need to work 6 days a week \n",
       "35       Really Dont Like Doing my Room Its So Borin...\n",
       "36       SOX!     Floyd was great, but relievers nee...\n",
       "37                              times by like a million\n",
       "38                    uploading pictures on friendster \n",
       "39       what type of a spaz downloads a virus? my b...\n",
       "40                 &amp;&amp;Fightiin Wiit The Babes...\n",
       "41      (: !!!!!! - so i wrote something last week. ...\n",
       "42                                        *enough said*\n",
       "43      ... Do I need to even say it?  Do I?  Well, ...\n",
       "44                      ... health class (what a joke!)\n",
       "45                 @ginaaa &lt;3 GO TO THE SHOW TONIGHT\n",
       "46      @Spiral_galaxy @YMPtweet  it really makes me...\n",
       "47     - All Time Low shall be my motivation for the...\n",
       "48      and the entertainment is over, someone compl...\n",
       "49      another year of Lakers .. That's neither mag...\n",
       "Name: SentimentText, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['SentimentText'].head(50)\n",
    "# a lot of slang! HTML entities '&lt;--', handles '@ginaa' etc we should remove... Tricky..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Could use TweetTokenizer from nltk module, but need to install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The following command must be run outside of the IPython shell:\n",
      "\n",
      "    $ pip install --user -U nltk\n",
      "\n",
      "The Python package manager (pip) can only be used from outside of IPython.\n",
      "Please reissue the `pip` command in a separate terminal or command prompt.\n",
      "\n",
      "See the Python documentation for more information on how to install packages:\n",
      "\n",
      "    https://docs.python.org/3/installing/\n"
     ]
    }
   ],
   "source": [
    "pip install --user -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-bff9e3503d97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTweetTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# lol just watch vids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "from nltk import TweetTokenizer \n",
    "# lol just watch vids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer.tokenize(df.iloc[0, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Or use own preprocessor instead of TweetTokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from html import unescape\n",
    "\n",
    "def preprocessor(doc):\n",
    "    return unescape(doc).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unescape('&lt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['ner', 'parser', 'tagger'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(doc):\n",
    "    return [word.lemma_ for word in nlp(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS_lemma = [word.lemma_ for word in nlp(' '.join(list(STOP_WORDS)))]\n",
    "STOP_WORDS_lemma = set(STOP_WORDS_lemma).union({',', '.', ';'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_model():\n",
    "    vectorizer = HashingVectorizer(preprocessor=preprocessor,\n",
    "                            # tokenizer=lemmatizer,\n",
    "                            alternate_sign=False,\n",
    "                            # ngram_range=(1, 2),\n",
    "                            stop_words=STOP_WORDS_lemma) # kernel dies even without removing:(\n",
    "    clf = MultinomialNB()\n",
    "    pipe = Pipeline([\n",
    "        ('vectorizer', vectorizer), ('classifier', clf)\n",
    "    ])\n",
    "    \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['SentimentText']\n",
    "y = df['Sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vectorizer',\n",
       "                 HashingVectorizer(alternate_sign=False, analyzer='word',\n",
       "                                   binary=False, decode_error='strict',\n",
       "                                   dtype=<class 'numpy.float64'>,\n",
       "                                   encoding='utf-8', input='content',\n",
       "                                   lowercase=True, n_features=1048576,\n",
       "                                   ngram_range=(1, 1), norm='l2',\n",
       "                                   preprocessor=<function preprocessor at 0x7f2f35ca0400>,\n",
       "                                   stop_words={',', '.', '3', ';', 'a', 'about',\n",
       "                                               'a...',\n",
       "                                               'afterwards', 'again', 'against',\n",
       "                                               'all', 'almost', 'alone',\n",
       "                                               'along', 'already', 'also',\n",
       "                                               'although', 'always', 'among',\n",
       "                                               'amongst', 'amount', 'and',\n",
       "                                               'another', 'any', 'anyhow',\n",
       "                                               'anyone', 'anything', 'anyway', ...},\n",
       "                                   strip_accents=None,\n",
       "                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                   tokenizer=None)),\n",
       "                ('classifier',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8059869077963305"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.score(X_train, y_train) #0.81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.769585997852548"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.score(X_test, y_test) # 0.77!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persisting Model\n",
    "Want to access model without retraining - pickle/dill file, dump to disc to upload later. Can do `partial_fit` with new data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_model():\n",
    "    model = construct_model()\n",
    "    \n",
    "    with gzip.open('sentiment_model.dill.gz', 'wb') as f: # write mode\n",
    "        dill.dump(pipe, f, recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '-': No such file or directory\r\n",
      "ls: cannot access 'alh': No such file or directory\r\n",
      "sentiment_model.dill.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls - alh sentiment_model.dill.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('sentiment_model.dill.gz', 'rb') as f: # read mode\n",
    "    model = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.769585997852548"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test) # 0.77 as before!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Could construct functions `ConstructModel` and `SerializeModel` to encapsualte the above process (Done)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next step:** Done ML part. Now want to load dill file to use model in web application! (Python Flask application)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flask App Local Development Summary\n",
    "\n",
    "### GET Requests\n",
    "\n",
    "### Using Model with Twitter API\n",
    "\n",
    "### POST Requests\n",
    "\n",
    "### Deployment to Web via Heroku\n",
    "Cloud platform to host web apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The following command must be run outside of the IPython shell:\n",
      "\n",
      "    $ pip install Flask\n",
      "\n",
      "The Python package manager (pip) can only be used from outside of IPython.\n",
      "Please reissue the `pip` command in a separate terminal or command prompt.\n",
      "\n",
      "See the Python documentation for more information on how to install packages:\n",
      "\n",
      "    https://docs.python.org/3/installing/\n"
     ]
    }
   ],
   "source": [
    "pip install Flask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
